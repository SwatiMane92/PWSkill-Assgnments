{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff69e0-d0d0-43b6-a291-2ff2686aac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Q1: Simple Linear Regression vs. Multiple Linear Regression\n",
    "\n",
    "# Example of Simple Linear Regression\n",
    "# Generate some example data\n",
    "np.random.seed(0)\n",
    "X_simple = np.random.rand(100, 1) * 10\n",
    "y_simple = 2 * X_simple + np.random.randn(100, 1) * 2\n",
    "\n",
    "# Fit Simple Linear Regression model\n",
    "model_simple = LinearRegression()\n",
    "model_simple.fit(X_simple, y_simple)\n",
    "\n",
    "# Plot Simple Linear Regression\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_simple, y_simple, color='blue', label='Data points')\n",
    "plt.plot(X_simple, model_simple.predict(X_simple), color='red', label='Fitted line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Example of Multiple Linear Regression\n",
    "# Generate some example data\n",
    "np.random.seed(0)\n",
    "X_multiple = np.random.rand(100, 3) * 10\n",
    "y_multiple = 3 * X_multiple[:, 0] + 2 * X_multiple[:, 1] - X_multiple[:, 2] + np.random.randn(100) * 2\n",
    "\n",
    "# Fit Multiple Linear Regression model\n",
    "model_multiple = LinearRegression()\n",
    "model_multiple.fit(X_multiple, y_multiple)\n",
    "\n",
    "# Q1 Summary\n",
    "simple_regression = \"\"\"\n",
    "Simple Linear Regression involves a single predictor variable and aims to model the relationship between that predictor and the target variable. For example, predicting house prices based on square footage alone.\n",
    "Multiple Linear Regression involves multiple predictor variables and models the relationship between them and the target variable. For example, predicting house prices based on square footage, number of bedrooms, and location.\n",
    "\"\"\"\n",
    "\n",
    "# Q2: Assumptions of Linear Regression\n",
    "\n",
    "# Checking assumptions (visualization and statistical tests are needed in practice)\n",
    "assumptions = \"\"\"\n",
    "1. Linearity: The relationship between predictors and the target is linear.\n",
    "2. Independence: Observations are independent of each other.\n",
    "3. Homoscedasticity: The residuals have constant variance.\n",
    "4. Normality of Residuals: Residuals are normally distributed.\n",
    "\n",
    "You can check these assumptions by:\n",
    "- Plotting residuals vs. fitted values to check for homoscedasticity.\n",
    "- Using Q-Q plots or Shapiro-Wilk test to check residual normality.\n",
    "- Checking for multicollinearity using VIF (Variance Inflation Factor).\n",
    "\"\"\"\n",
    "\n",
    "# Q3: Interpreting Slope and Intercept\n",
    "\n",
    "# Example interpretation\n",
    "intercept = model_simple.intercept_[0]\n",
    "slope = model_simple.coef_[0][0]\n",
    "example_interpretation = f\"\"\"\n",
    "In the simple linear regression model: y = {intercept:.2f} + {slope:.2f} * X\n",
    "- The intercept (b0) is {intercept:.2f}. It represents the expected value of y when X = 0.\n",
    "- The slope (b1) is {slope:.2f}. It indicates the change in y for a one-unit change in X.\n",
    "For example, if we predict house prices, the intercept is the price when the size is zero, and the slope represents how much the price increases for each additional square foot.\n",
    "\"\"\"\n",
    "\n",
    "# Q4: Gradient Descent\n",
    "\n",
    "gradient_descent = \"\"\"\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost function by iteratively moving in the direction of the steepest descent. In machine learning, it is used to find the optimal parameters (weights) for the model.\n",
    "The process involves:\n",
    "1. Initializing parameters randomly.\n",
    "2. Calculating the gradient of the cost function.\n",
    "3. Updating parameters in the opposite direction of the gradient.\n",
    "4. Repeating until convergence.\n",
    "\"\"\"\n",
    "\n",
    "# Q5: Multiple Linear Regression Model\n",
    "\n",
    "multiple_linear_regression = \"\"\"\n",
    "Multiple Linear Regression involves modeling the relationship between multiple predictor variables and a target variable. Unlike Simple Linear Regression, which uses one predictor, Multiple Linear Regression accounts for the effects of multiple predictors simultaneously.\n",
    "\"\"\"\n",
    "\n",
    "# Q6: Multicollinearity in Multiple Linear Regression\n",
    "\n",
    "# Example to detect multicollinearity\n",
    "X_with_const = add_constant(X_multiple)\n",
    "vif = pd.DataFrame()\n",
    "vif['Variable'] = X_with_const.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n",
    "\n",
    "multicollinearity_summary = \"\"\"\n",
    "Multicollinearity occurs when predictor variables are highly correlated with each other, which can lead to unstable estimates of coefficients.\n",
    "- Detection: Use VIF (Variance Inflation Factor). VIF values greater than 10 indicate significant multicollinearity.\n",
    "- Addressing: Remove highly correlated predictors, combine predictors, or use techniques like Principal Component Analysis (PCA).\n",
    "\"\"\"\n",
    "\n",
    "# Q7: Polynomial Regression Model\n",
    "\n",
    "# Example of Polynomial Regression\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X_simple)\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly, y_simple)\n",
    "\n",
    "# Plot Polynomial Regression\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_simple, y_simple, color='blue', label='Data points')\n",
    "plt.plot(X_simple, model_poly.predict(poly.transform(X_simple)), color='green', label='Polynomial fit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Polynomial Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Q7 Summary\n",
    "polynomial_regression = \"\"\"\n",
    "Polynomial Regression extends Linear Regression by including polynomial terms of the predictor variables. This allows for modeling non-linear relationships.\n",
    "For example, predicting house prices with a quadratic term for square footage.\n",
    "\"\"\"\n",
    "\n",
    "# Q8: Advantages and Disadvantages of Polynomial Regression\n",
    "\n",
    "polynomial_advantages_disadvantages = \"\"\"\n",
    "Advantages:\n",
    "1. Can capture non-linear relationships between predictors and the target.\n",
    "2. Allows for more flexibility in modeling.\n",
    "\n",
    "Disadvantages:\n",
    "1. Can lead to overfitting, especially with high-degree polynomials.\n",
    "2. More complex models can be harder to interpret.\n",
    "\n",
    "Use Polynomial Regression when you suspect a non-linear relationship between predictors and the target variable.\n",
    "\"\"\"\n",
    "\n",
    "# Display results\n",
    "print(\"Q1: Simple vs Multiple Linear Regression\")\n",
    "print(simple_regression)\n",
    "\n",
    "print(\"\\nQ2: Assumptions of Linear Regression\")\n",
    "print(assumptions)\n",
    "\n",
    "print(\"\\nQ3: Interpreting Slope and Intercept\")\n",
    "print(example_interpretation)\n",
    "\n",
    "print(\"\\nQ4: Gradient Descent\")\n",
    "print(gradient_descent)\n",
    "\n",
    "print(\"\\nQ5: Multiple Linear Regression Model\")\n",
    "print(multiple_linear_regression)\n",
    "\n",
    "print(\"\\nQ6: Multicollinearity in Multiple Linear Regression\")\n",
    "print(multicollinearity_summary)\n",
    "print(vif)\n",
    "\n",
    "print(\"\\nQ7: Polynomial Regression Model\")\n",
    "print(polynomial_regression)\n",
    "\n",
    "print(\"\\nQ8: Advantages and Disadvantages of Polynomial Regression\")\n",
    "print(polynomial_advantages_disadvantages)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
