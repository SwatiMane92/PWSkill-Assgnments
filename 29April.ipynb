{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f0d20-4614-4d8d-8adc-92c393e138e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "print(\"Loading dataset...\")\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "target = pd.Series(iris.target, name='target')\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Perform Hierarchical Clustering\n",
    "print(\"Performing Hierarchical Clustering...\")\n",
    "Z = linkage(data_scaled, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(Z, labels=target.values, leaf_rotation=90, leaf_font_size=12)\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Determine the optimal number of clusters using a cut-off on the dendrogram\n",
    "max_d = 7.5  # Example cutoff distance for clusters\n",
    "clusters = fcluster(Z, max_d, criterion='distance')\n",
    "data['Cluster'] = clusters\n",
    "\n",
    "# Summary of clustering results\n",
    "print(\"Cluster distribution:\")\n",
    "print(data['Cluster'].value_counts())\n",
    "\n",
    "print(\"\"\"\n",
    "Q1. Hierarchical Clustering:\n",
    "Hierarchical clustering is a clustering technique that builds a hierarchy of clusters. Unlike K-Means, which requires specifying the number of clusters in advance, hierarchical clustering does not require this, and the number of clusters can be determined after the algorithm has run.\n",
    "\n",
    "**Differences from Other Clustering Techniques**:\n",
    "- **K-Means**: Partitional method that requires specifying K clusters. Hierarchical clustering creates a tree of clusters without needing a predefined number of clusters.\n",
    "- **DBSCAN**: Density-based method that can find clusters of arbitrary shapes. Hierarchical clustering builds a nested hierarchy of clusters.\n",
    "\n",
    "Q2. Types of Hierarchical Clustering Algorithms:\n",
    "1. **Agglomerative Clustering**:\n",
    "   - **Bottom-Up Approach**: Starts with each data point as its own cluster and merges the closest pairs of clusters until all points belong to a single cluster or a stopping criterion is met.\n",
    "   - **Linkage Methods**: Determines the distance between clusters (e.g., single-linkage, complete-linkage, average-linkage).\n",
    "\n",
    "2. **Divisive Clustering**:\n",
    "   - **Top-Down Approach**: Starts with a single cluster containing all data points and recursively splits the cluster into smaller clusters until each point is in its own cluster or a stopping criterion is met.\n",
    "   - **Less common** compared to agglomerative clustering.\n",
    "\n",
    "Q3. Distance Metrics for Clustering:\n",
    "- **Euclidean Distance**: Standard metric for continuous data, calculates the straight-line distance between two points.\n",
    "- **Manhattan Distance**: Calculates the distance as the sum of absolute differences between coordinates.\n",
    "- **Cosine Similarity**: Measures the cosine of the angle between two vectors; often used in text data.\n",
    "\n",
    "Q4. Determining the Optimal Number of Clusters:\n",
    "**Methods**:\n",
    "1. **Dendrogram Analysis**: Examine the dendrogram and choose the number of clusters based on the largest vertical distance (cut-off point).\n",
    "2. **Silhouette Score**: Measure how similar a point is to its own cluster compared to other clusters.\n",
    "3. **Elbow Method**: Plot the within-cluster variance against the number of clusters and look for an \"elbow\" point.\n",
    "\n",
    "Q5. Dendrograms:\n",
    "- **Definition**: A dendrogram is a tree-like diagram that shows the arrangement of clusters produced by hierarchical clustering.\n",
    "- **Usage**: Helps in visualizing the clustering process and determining the number of clusters by cutting the dendrogram at a certain height.\n",
    "\n",
    "Q6. Hierarchical Clustering for Different Data Types:\n",
    "- **Numerical Data**: Use Euclidean or Manhattan distance.\n",
    "- **Categorical Data**: Use methods like Jaccard or Hamming distance. Convert categorical data into numerical formats using encoding techniques.\n",
    "\n",
    "Q7. Identifying Outliers:\n",
    "- **Outliers**: Can be identified as data points that form their own clusters or are far away from other points in the dendrogram.\n",
    "- **Method**: Use the dendrogram to identify isolated clusters or clusters with very few members.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
