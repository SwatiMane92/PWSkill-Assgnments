{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2fc863-144e-4d76-91f1-d40ed992aacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1. Building the pipeline\n",
      "Accuracy of the Random Forest model: 0.82\n",
      "\n",
      "Interpretation and Suggestions:\n",
      "1. The pipeline includes automated feature selection and handles missing values and feature scaling for numerical features, as well as one-hot encoding for categorical features.\n",
      "2. Possible improvements could include tuning hyperparameters, trying different feature selection methods, or using other classification algorithms.\n",
      "\n",
      "Q2. Building the pipeline with Voting Classifier\n",
      "Accuracy of the Voting Classifier: 0.84\n",
      "\n",
      "Interpretation and Suggestions:\n",
      "1. The Voting Classifier combines predictions from both Random Forest and Logistic Regression models.\n",
      "2. It can provide better performance by leveraging the strengths of different classifiers.\n",
      "3. Evaluate whether the soft voting improves accuracy compared to individual models.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "url = 'https://drive.google.com/uc?id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Assume 'target' is the name of the target column\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Q1. Build the pipeline\n",
    "\n",
    "print(\"Q1. Building the pipeline\")\n",
    "\n",
    "# Automated feature selection\n",
    "feature_selector = SelectKBest(score_func=f_classif, k='all')\n",
    "\n",
    "# Numerical pipeline\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline with feature selection and classification\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', feature_selector),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the Random Forest model: {accuracy:.2f}\")\n",
    "\n",
    "# Interpretation and suggestions\n",
    "print(\"\\nInterpretation and Suggestions:\")\n",
    "print(\"1. The pipeline includes automated feature selection and handles missing values and feature scaling for numerical features, as well as one-hot encoding for categorical features.\")\n",
    "print(\"2. Possible improvements could include tuning hyperparameters, trying different feature selection methods, or using other classification algorithms.\")\n",
    "\n",
    "# Q2. Build a pipeline with a Random Forest Classifier and Logistic Regression, then use a Voting Classifier\n",
    "\n",
    "print(\"\\nQ2. Building the pipeline with Voting Classifier\")\n",
    "\n",
    "# Define classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "lr_classifier = LogisticRegression()\n",
    "\n",
    "# Voting classifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', rf_classifier),\n",
    "    ('lr', lr_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Full pipeline with voting classifier\n",
    "voting_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', feature_selector),\n",
    "    ('voting_clf', voting_clf)\n",
    "])\n",
    "\n",
    "# Train the voting classifier\n",
    "voting_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the voting classifier\n",
    "y_pred_voting = voting_pipeline.predict(X_test)\n",
    "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
    "print(f\"Accuracy of the Voting Classifier: {accuracy_voting:.2f}\")\n",
    "\n",
    "# Interpretation and suggestions\n",
    "print(\"\\nInterpretation and Suggestions:\")\n",
    "print(\"1. The Voting Classifier combines predictions from both Random Forest and Logistic Regression models.\")\n",
    "print(\"2. It can provide better performance by leveraging the strengths of different classifiers.\")\n",
    "print(\"3. Evaluate whether the soft voting improves accuracy compared to individual models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4929958-24b4-469b-b241-99542b45d7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
