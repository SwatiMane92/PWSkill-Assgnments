{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f045857-f4f6-47a7-ad88-03a060d8e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Q1: Difference between Linear Regression and Logistic Regression\n",
    "linear_vs_logistic_regression = \"\"\"\n",
    "**Linear Regression**:\n",
    "- **Purpose**: Predicts a continuous dependent variable based on one or more independent variables.\n",
    "- **Output**: Continuous values.\n",
    "- **Example**: Predicting house prices based on features like size, location, and number of rooms.\n",
    "\n",
    "**Logistic Regression**:\n",
    "- **Purpose**: Predicts the probability of a binary outcome (class) based on one or more independent variables.\n",
    "- **Output**: Probability between 0 and 1, which is used to classify into one of two classes.\n",
    "- **Example**: Predicting whether a customer will buy a product (Yes/No) based on their age, income, and browsing history.\n",
    "\"\"\"\n",
    "\n",
    "# Q2: Cost Function in Logistic Regression and Optimization\n",
    "logistic_cost_function = \"\"\"\n",
    "**Cost Function**:\n",
    "- **Cost Function**: Logistic Regression uses the **Log-Loss** or **Binary Cross-Entropy** as its cost function.\n",
    "- **Formula**: Cost = - (y * log(p) + (1 - y) * log(1 - p)), where y is the actual label and p is the predicted probability.\n",
    "\n",
    "**Optimization**:\n",
    "- **Optimization Technique**: The cost function is minimized using optimization algorithms such as Gradient Descent.\n",
    "- **Gradient Descent**: Iteratively updates the model parameters to find the minimum cost.\n",
    "\"\"\"\n",
    "\n",
    "# Q3: Regularization in Logistic Regression\n",
    "regularization_logistic_regression = \"\"\"\n",
    "**Regularization**:\n",
    "- **Purpose**: To prevent overfitting by adding a penalty to the cost function for large coefficients.\n",
    "- **Types**:\n",
    "  - **L1 Regularization (Lasso)**: Adds the absolute value of the coefficients to the cost function. Helps with feature selection.\n",
    "  - **L2 Regularization (Ridge)**: Adds the squared value of the coefficients to the cost function. Helps to shrink the coefficients.\n",
    "\n",
    "**Impact**:\n",
    "- Reduces the model complexity and improves generalization to unseen data.\n",
    "\"\"\"\n",
    "\n",
    "# Q4: ROC Curve in Logistic Regression\n",
    "roc_curve_explanation = \"\"\"\n",
    "**ROC Curve (Receiver Operating Characteristic Curve)**:\n",
    "- **Purpose**: To evaluate the performance of a binary classification model by plotting the true positive rate (sensitivity) against the false positive rate (1 - specificity).\n",
    "- **AUC (Area Under the Curve)**: Represents the overall performance. AUC of 1 indicates a perfect model, and AUC of 0.5 indicates a random model.\n",
    "\n",
    "**How to Use**:\n",
    "- **Calculate ROC Curve**: Plot the curve based on the true positive rate and false positive rate at various thresholds.\n",
    "- **Evaluate Performance**: Higher AUC values represent better model performance.\n",
    "\"\"\"\n",
    "\n",
    "# Q5: Feature Selection Techniques in Logistic Regression\n",
    "feature_selection_techniques = \"\"\"\n",
    "**Common Techniques**:\n",
    "1. **Recursive Feature Elimination (RFE)**: Iteratively builds the model and removes the weakest features.\n",
    "2. **L1 Regularization**: Lasso regularization can shrink some coefficients to zero, effectively performing feature selection.\n",
    "3. **Univariate Selection**: Select features based on statistical tests (e.g., chi-squared test).\n",
    "4. **Feature Importance from Tree-Based Models**: Use models like Random Forest to rank features by importance.\n",
    "\n",
    "**Impact**:\n",
    "- Improves model performance by removing irrelevant or redundant features.\n",
    "- Reduces overfitting and enhances model interpretability.\n",
    "\"\"\"\n",
    "\n",
    "# Q6: Handling Imbalanced Datasets in Logistic Regression\n",
    "imbalanced_datasets_strategies = \"\"\"\n",
    "**Strategies**:\n",
    "1. **Resampling Techniques**:\n",
    "   - **Oversampling**: Increase the number of instances in the minority class (e.g., SMOTE).\n",
    "   - **Undersampling**: Decrease the number of instances in the majority class.\n",
    "\n",
    "2. **Class Weights**:\n",
    "   - **Adjust Class Weights**: Increase the weight of the minority class in the loss function to penalize misclassifications more heavily.\n",
    "\n",
    "3. **Ensemble Methods**:\n",
    "   - **Balanced Random Forest**: Combine multiple balanced trees to improve performance on imbalanced data.\n",
    "\n",
    "4. **Threshold Adjustment**:\n",
    "   - **Adjust Decision Threshold**: Change the threshold for classification to better balance precision and recall.\n",
    "\"\"\"\n",
    "\n",
    "# Q7: Common Issues in Logistic Regression\n",
    "logistic_regression_issues = \"\"\"\n",
    "**Common Issues and Challenges**:\n",
    "1. **Multicollinearity**:\n",
    "   - **Problem**: High correlation between independent variables can make it difficult to determine the individual effect of each variable.\n",
    "   - **Solution**: Use techniques such as Variance Inflation Factor (VIF) to detect multicollinearity and consider removing or combining correlated features.\n",
    "\n",
    "2. **Overfitting**:\n",
    "   - **Problem**: The model may fit the training data too closely and perform poorly on new data.\n",
    "   - **Solution**: Use regularization techniques (L1/L2), cross-validation, and feature selection to address overfitting.\n",
    "\n",
    "3. **Imbalanced Data**:\n",
    "   - **Problem**: Class imbalance can lead to biased predictions.\n",
    "   - **Solution**: Apply resampling techniques, adjust class weights, or use different evaluation metrics.\n",
    "\"\"\"\n",
    "\n",
    "# Example of ROC Curve using synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Display results\n",
    "print(\"Q1: Difference between Linear Regression and Logistic Regression\")\n",
    "print(linear_vs_logistic_regression)\n",
    "\n",
    "print(\"\\nQ2: Cost Function in Logistic Regression and Optimization\")\n",
    "print(logistic_cost_function)\n",
    "\n",
    "print(\"\\nQ3: Regularization in Logistic Regression\")\n",
    "print(regularization_logistic_regression)\n",
    "\n",
    "print(\"\\nQ4: ROC Curve in Logistic Regression\")\n",
    "print(roc_curve_explanation)\n",
    "\n",
    "print(\"\\nQ5: Feature Selection Techniques in Logistic Regression\")\n",
    "print(feature_selection_techniques)\n",
    "\n",
    "print(\"\\nQ6: Handling Imbalanced Datasets in Logistic Regression\")\n",
    "print(imbalanced_datasets_strategies)\n",
    "\n",
    "print(\"\\nQ7: Common Issues in Logistic Regression\")\n",
    "print(logistic_regression_issues)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
