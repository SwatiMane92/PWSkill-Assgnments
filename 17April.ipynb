{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8818b-1e22-4696-8d30-9cf3e5bfe78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Create a simple dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the gradient boosting regression class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.init_prediction = np.mean(y_train)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions with the mean of y\n",
    "        self.predictions = np.full_like(y, self.init_prediction, dtype=np.float64)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - self.predictions\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals)\n",
    "            prediction = model.predict(X)\n",
    "            self.predictions += self.learning_rate * prediction\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.full(X.shape[0], self.init_prediction, dtype=np.float64)\n",
    "        for model in self.models:\n",
    "            prediction += self.learning_rate * model.predict(X)\n",
    "        return prediction\n",
    "\n",
    "# Initialize and train the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n",
    "\n",
    "# Hyperparameter tuning using Grid Search\n",
    "from sklearn.ensemble import GradientBoostingRegressor as SklearnGBR\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SklearnGBR(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best Mean Squared Error: {mse_best:.2f}\")\n",
    "print(f\"Best R-squared: {r2_best:.2f}\")\n",
    "\n",
    "# Explanations\n",
    "print(\"\\nQ1. What is Gradient Boosting Regression?\")\n",
    "print(\"Gradient Boosting Regression is an ensemble learning technique that builds a model by combining the predictions of multiple weak learners, typically decision trees, to improve predictive performance. It sequentially fits new models to the residuals of the combined predictions of previous models.\")\n",
    "\n",
    "print(\"\\nQ4. What is a weak learner in Gradient Boosting?\")\n",
    "print(\"A weak learner is a model that performs slightly better than random chance. In Gradient Boosting, weak learners are usually simple models, such as shallow decision trees, that are combined to create a strong model.\")\n",
    "\n",
    "print(\"\\nQ5. What is the intuition behind the Gradient Boosting algorithm?\")\n",
    "print(\"The intuition behind Gradient Boosting is to iteratively improve the model by focusing on correcting the errors of the previous models. It does this by fitting new models to the residuals (errors) of the combined predictions of all previously trained models.\")\n",
    "\n",
    "print(\"\\nQ6. How does Gradient Boosting algorithm build an ensemble of weak learners?\")\n",
    "print(\"Gradient Boosting builds an ensemble of weak learners by training them sequentially. Each new model is trained to predict the residuals of the combined predictions of the previous models. The final model is an aggregate of all the weak learners' predictions, weighted by their learning rates.\")\n",
    "\n",
    "print(\"\\nQ7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\")\n",
    "print(\"1. Initialize the model with a simple base prediction (e.g., mean of the target values).\")\n",
    "print(\"2. Compute the residuals (errors) between the actual target values and the current model's predictions.\")\n",
    "print(\"3. Fit a new model to the residuals.\")\n",
    "print(\"4. Update the predictions by adding the predictions of the new model, scaled by a learning rate.\")\n",
    "print(\"5. Repeat the process for a predefined number of iterations or until no significant improvement is observed.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
