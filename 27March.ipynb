{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816a076-7593-4fbc-9968-128d82d3e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data generation for demonstration\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Q1: R-squared in Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "r2_summary = f\"\"\"\n",
    "R-squared (R²) measures the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "- Calculated as: 1 - (Sum of Squared Errors / Total Sum of Squares)\n",
    "- Represents the proportion of variance explained by the model.\n",
    "- R² ranges from 0 to 1. A value closer to 1 indicates a better fit.\n",
    "\"\"\"\n",
    "\n",
    "# Q2: Adjusted R-squared\n",
    "def adjusted_r2(r2, n, k):\n",
    "    \"\"\"Calculate adjusted R-squared\"\"\"\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - k - 1)\n",
    "\n",
    "n = len(y_test)\n",
    "k = X.shape[1]  # Number of predictors\n",
    "adj_r2 = adjusted_r2(r2, n, k)\n",
    "\n",
    "adj_r2_summary = f\"\"\"\n",
    "Adjusted R-squared adjusts the R-squared value for the number of predictors in the model.\n",
    "- It accounts for the number of predictors (k) and sample size (n).\n",
    "- More appropriate than R² when comparing models with different numbers of predictors.\n",
    "\"\"\"\n",
    "\n",
    "# Q3: When to use Adjusted R-squared\n",
    "adjusted_r2_usage = \"\"\"\n",
    "Adjusted R-squared is more appropriate when:\n",
    "- Comparing models with different numbers of predictors.\n",
    "- Evaluating model performance in the presence of multiple predictors.\n",
    "- Helps to avoid overfitting by penalizing the addition of non-significant predictors.\n",
    "\"\"\"\n",
    "\n",
    "# Q4: RMSE, MSE, and MAE in Regression Analysis\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "metrics_summary = f\"\"\"\n",
    "- RMSE (Root Mean Squared Error): √MSE. Represents the standard deviation of the residuals. Sensitive to large errors.\n",
    "- MSE (Mean Squared Error): Average of the squared differences between predicted and actual values. Penalizes larger errors more than MAE.\n",
    "- MAE (Mean Absolute Error): Average of absolute differences between predicted and actual values. Provides a linear score without amplifying the effect of outliers.\n",
    "\"\"\"\n",
    "\n",
    "# Q5: Advantages and Disadvantages of RMSE, MSE, and MAE\n",
    "metrics_adv_dis = \"\"\"\n",
    "Advantages:\n",
    "- RMSE: Useful for assessing the variance of residuals. Penalizes large errors more.\n",
    "- MSE: Simple to compute and interpret. Suitable for cases where large errors are particularly undesirable.\n",
    "- MAE: Easy to understand and less sensitive to outliers compared to RMSE and MSE.\n",
    "\n",
    "Disadvantages:\n",
    "- RMSE: Sensitive to outliers; may give a skewed perspective if outliers are present.\n",
    "- MSE: Squaring errors makes it sensitive to large deviations, which might not always be desirable.\n",
    "- MAE: Less sensitive to large errors, which might be a disadvantage if you want to penalize outliers more heavily.\n",
    "\"\"\"\n",
    "\n",
    "# Q6: Lasso vs. Ridge Regularization\n",
    "lasso_model = Lasso(alpha=0.5)\n",
    "ridge_model = Ridge(alpha=0.1)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "lasso_summary = \"\"\"\n",
    "Lasso Regularization (L1): Adds absolute value of coefficients to the loss function, which can lead to sparsity (some coefficients become zero). Useful for feature selection.\n",
    "Ridge Regularization (L2): Adds squared value of coefficients to the loss function, which shrinks coefficients but doesn’t force them to zero. Useful for regularizing the model without eliminating features.\n",
    "\"\"\"\n",
    "\n",
    "# Q7: How Regularized Linear Models Help to Prevent Overfitting\n",
    "regularization_summary = \"\"\"\n",
    "Regularized linear models help prevent overfitting by adding a penalty to the magnitude of coefficients, which discourages overly complex models. \n",
    "For example, Ridge regression can reduce the impact of less significant predictors, and Lasso can perform feature selection by setting some coefficients to zero.\n",
    "\"\"\"\n",
    "\n",
    "# Q8: Limitations of Regularized Linear Models\n",
    "regularization_limitations = \"\"\"\n",
    "Limitations:\n",
    "- Regularization might not work well for models where interaction terms or polynomial terms are important.\n",
    "- Over-regularization can lead to underfitting and loss of important information.\n",
    "- Choice of regularization type (L1 vs. L2) depends on the problem; no one-size-fits-all solution.\n",
    "\"\"\"\n",
    "\n",
    "# Q9: Comparing RMSE and MAE\n",
    "comparison_rmse_mae = f\"\"\"\n",
    "Model A (RMSE = 10) vs. Model B (MAE = 8):\n",
    "- RMSE gives more weight to large errors and is sensitive to outliers.\n",
    "- MAE provides a more balanced measure of error magnitude.\n",
    "- The choice depends on whether large errors are more critical or if a more balanced error measure is desired.\n",
    "\"\"\"\n",
    "\n",
    "# Q10: Comparing Ridge and Lasso Regularization\n",
    "ridge_lasso_comparison = f\"\"\"\n",
    "Model A (Ridge, alpha=0.1) vs. Model B (Lasso, alpha=0.5):\n",
    "- Ridge regularization shrinks coefficients but keeps all features, good for handling multicollinearity.\n",
    "- Lasso regularization can set some coefficients to zero, performing feature selection.\n",
    "- The choice depends on whether you need feature selection (Lasso) or just regularization (Ridge).\n",
    "\"\"\"\n",
    "\n",
    "# Display results\n",
    "print(\"Q1: R-squared\")\n",
    "print(r2_summary)\n",
    "print(f\"R-squared value: {r2:.3f}\")\n",
    "\n",
    "print(\"\\nQ2: Adjusted R-squared\")\n",
    "print(adj_r2_summary)\n",
    "print(f\"Adjusted R-squared value: {adj_r2:.3f}\")\n",
    "\n",
    "print(\"\\nQ3: When to Use Adjusted R-squared\")\n",
    "print(adjusted_r2_usage)\n",
    "\n",
    "print(\"\\nQ4: RMSE, MSE, and MAE\")\n",
    "print(metrics_summary)\n",
    "print(f\"RMSE: {rmse:.3f}, MSE: {mse:.3f}, MAE: {mae:.3f}\")\n",
    "\n",
    "print(\"\\nQ5: Advantages and Disadvantages of RMSE, MSE, and MAE\")\n",
    "print(metrics_adv_dis)\n",
    "\n",
    "print(\"\\nQ6: Lasso vs. Ridge Regularization\")\n",
    "print(lasso_summary)\n",
    "\n",
    "print(\"\\nQ7: How Regularized Linear Models Help to Prevent Overfitting\")\n",
    "print(regularization_summary)\n",
    "\n",
    "print(\"\\nQ8: Limitations of Regularized Linear Models\")\n",
    "print(regularization_limitations)\n",
    "\n",
    "print(\"\\nQ9: Comparing RMSE and MAE\")\n",
    "print(comparison_rmse_mae)\n",
    "\n",
    "print(\"\\nQ10: Comparing Ridge and Lasso Regularization\")\n",
    "print(ridge_lasso_comparison)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
