{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d16e2-91d0-4303-b37c-05d9b71bf979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "print(\"Loading dataset...\")\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "target = pd.Series(iris.target, name='target')\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "print(\"Performing K-Means clustering...\")\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# Evaluate clustering performance\n",
    "silhouette_avg = silhouette_score(data_scaled, clusters)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data_scaled[:, 0], data_scaled[:, 1], c=clusters, cmap='viridis', edgecolor='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('K-Means Clustering Results')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Summary of clustering results\n",
    "data['Cluster'] = clusters\n",
    "print(\"Cluster distribution:\")\n",
    "print(data['Cluster'].value_counts())\n",
    "\n",
    "print(\"\"\"\n",
    "Q1. Different Types of Clustering Algorithms:\n",
    "1. **K-Means Clustering**: Partitional clustering method that aims to partition data into K clusters, minimizing the variance within each cluster.\n",
    "2. **Hierarchical Clustering**: Builds a hierarchy of clusters either by agglomeration (bottom-up approach) or divisive (top-down approach).\n",
    "3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: Clusters based on density, allowing for clusters of arbitrary shapes and handling noise.\n",
    "4. **Mean Shift Clustering**: A non-parametric algorithm that shifts data points to the mode of the data distribution.\n",
    "5. **Gaussian Mixture Models (GMMs)**: Assumes data is generated from a mixture of several Gaussian distributions and uses Expectation-Maximization to find the parameters.\n",
    "\n",
    "Q2. K-Means Clustering:\n",
    "K-Means is an iterative algorithm that partitions data into K clusters. It works as follows:\n",
    "1. Initialize K cluster centroids randomly.\n",
    "2. Assign each data point to the nearest centroid.\n",
    "3. Recalculate centroids as the mean of the data points assigned to each cluster.\n",
    "4. Repeat steps 2 and 3 until centroids no longer change.\n",
    "\n",
    "Q3. Advantages and Limitations of K-Means Clustering:\n",
    "**Advantages**:\n",
    "- Simple and easy to implement.\n",
    "- Efficient with large datasets.\n",
    "- Works well when clusters are spherical and of similar size.\n",
    "\n",
    "**Limitations**:\n",
    "- Requires specifying the number of clusters K in advance.\n",
    "- Sensitive to the initial placement of centroids.\n",
    "- Assumes clusters are of similar size and density.\n",
    "\n",
    "Q4. Determining the Optimal Number of Clusters:\n",
    "**Methods**:\n",
    "1. **Elbow Method**: Plot the sum of squared distances from points to their assigned cluster centroids for different values of K and look for an \"elbow\" point.\n",
    "2. **Silhouette Score**: Measure how similar a data point is to its own cluster compared to other clusters.\n",
    "3. **Gap Statistic**: Compare the total within-cluster variation for different values of K with their expected values under null reference distribution.\n",
    "\n",
    "Q5. Applications of K-Means Clustering:\n",
    "1. **Customer Segmentation**: Group customers with similar purchasing behavior.\n",
    "2. **Image Compression**: Reduce the number of colors in an image by clustering similar colors.\n",
    "3. **Anomaly Detection**: Identify unusual patterns in data by clustering normal data points and flagging outliers.\n",
    "\n",
    "Q6. Interpreting K-Means Clustering Output:\n",
    "- **Cluster Centroids**: Represent the mean feature values of each cluster.\n",
    "- **Cluster Distribution**: Shows how data points are distributed among clusters.\n",
    "- **Cluster Characteristics**: Analyze the features of each cluster to gain insights into different groups within the data.\n",
    "\n",
    "Q7. Challenges and Solutions in K-Means Clustering:\n",
    "**Challenges**:\n",
    "- **Choosing K**: Difficult to determine the optimal number of clusters.\n",
    "- **Initialization Sensitivity**: Different initializations can lead to different results.\n",
    "\n",
    "**Solutions**:\n",
    "- **Use K-Means++ Initialization**: Improve centroid initialization.\n",
    "- **Run Multiple Trials**: Run K-Means multiple times with different initializations and choose the best result.\n",
    "\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
