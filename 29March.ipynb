{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d80f30-5773-4f46-9c88-fa68f2891340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Sample data generation for demonstration\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 10)  # 10 features\n",
    "y = np.dot(X, np.random.rand(10)) + np.random.randn(100) * 0.5\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Lasso Regression with cross-validation for lambda tuning\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Set up a Lasso regression model with GridSearchCV for lambda tuning\n",
    "lasso_model = Lasso()\n",
    "alpha_range = np.logspace(-4, 4, 100)  # Range of lambda values to test\n",
    "param_grid = {'alpha': alpha_range}\n",
    "grid_search = GridSearchCV(lasso_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best lambda value from GridSearchCV\n",
    "best_lambda = grid_search.best_params_['alpha']\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Q1: Lasso Regression Overview\n",
    "lasso_regression_summary = \"\"\"\n",
    "Lasso Regression (L1 Regularization) adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function. This penalty term is λ * sum(|coefficients|).\n",
    "- Unlike Ordinary Least Squares (OLS) Regression, which does not include any regularization, Lasso Regression shrinks the coefficients and can force some to be exactly zero, effectively performing feature selection.\n",
    "- It differs from Ridge Regression, which uses L2 regularization and does not set coefficients to zero.\n",
    "\"\"\"\n",
    "\n",
    "# Q2: Advantage of Lasso Regression in Feature Selection\n",
    "feature_selection_advantage = \"\"\"\n",
    "The main advantage of Lasso Regression in feature selection is its ability to reduce some coefficients to zero, effectively excluding those features from the model.\n",
    "- This is particularly useful for high-dimensional datasets where feature selection is crucial to improve model performance and interpretability.\n",
    "- By setting some coefficients to zero, Lasso Regression helps in identifying the most important features.\n",
    "\"\"\"\n",
    "\n",
    "# Q3: Interpreting Coefficients of a Lasso Regression Model\n",
    "coefficient_interpretation = \"\"\"\n",
    "In Lasso Regression, coefficients represent the impact of each feature on the target variable, similar to OLS Regression.\n",
    "- However, due to regularization, some coefficients can be exactly zero, indicating that those features are not included in the model.\n",
    "- Non-zero coefficients reflect the importance of the corresponding features in predicting the target variable.\n",
    "\"\"\"\n",
    "\n",
    "# Q4: Tuning Parameters in Lasso Regression\n",
    "tuning_parameters = \"\"\"\n",
    "The primary tuning parameter in Lasso Regression is λ (alpha), which controls the strength of the regularization.\n",
    "- Higher values of λ increase the penalty on the coefficients, leading to more coefficients being shrunk to zero and potentially more features being excluded.\n",
    "- Lower values of λ reduce the penalty, allowing more features to remain in the model and potentially increasing the risk of overfitting.\n",
    "- The optimal value of λ can be chosen using techniques like cross-validation.\n",
    "\"\"\"\n",
    "\n",
    "# Q5: Lasso Regression for Non-Linear Problems\n",
    "non_linear_regression_summary = \"\"\"\n",
    "Lasso Regression is inherently a linear regression technique and cannot directly handle non-linear relationships.\n",
    "- However, you can extend Lasso to non-linear problems by using polynomial features or other feature transformations to capture non-linear relationships.\n",
    "- After transforming the features, Lasso can be applied to the transformed feature set to perform regularization and feature selection.\n",
    "\"\"\"\n",
    "\n",
    "# Q6: Difference Between Ridge and Lasso Regression\n",
    "ridge_lasso_comparison = \"\"\"\n",
    "Ridge Regression (L2 Regularization) adds a penalty equal to the square of the magnitude of coefficients to the loss function, which helps to reduce their size but does not set them to zero.\n",
    "- Lasso Regression (L1 Regularization) adds a penalty equal to the absolute value of the coefficients, which can shrink some coefficients to zero and effectively perform feature selection.\n",
    "- Ridge is preferred when dealing with multicollinearity and when all features are expected to have some impact on the target, while Lasso is useful for sparse models and feature selection.\n",
    "\"\"\"\n",
    "\n",
    "# Q7: Handling Multicollinearity with Lasso Regression\n",
    "multicollinearity_summary = \"\"\"\n",
    "Lasso Regression can handle multicollinearity by penalizing large coefficients, which helps to reduce the impact of correlated features.\n",
    "- By forcing some coefficients to zero, Lasso effectively selects a subset of features, which can reduce the effects of multicollinearity and improve model stability.\n",
    "- However, if multicollinearity is severe, Lasso might not always completely resolve the issue, and additional techniques or regularization methods might be needed.\n",
    "\"\"\"\n",
    "\n",
    "# Q8: Choosing the Optimal Regularization Parameter (lambda)\n",
    "optimal_lambda_selection = f\"\"\"\n",
    "The optimal value of the regularization parameter λ (alpha) in Lasso Regression is chosen based on:\n",
    "1. Cross-Validation: Testing various lambda values using cross-validation to identify the one that minimizes the mean squared error.\n",
    "2. Grid Search: Systematic search over a range of lambda values to find the best performing model.\n",
    "3. Regularization Path Algorithms: Efficiently computes the entire path of regularization solutions.\n",
    "Best lambda value from GridSearchCV: {best_lambda:.4f}\n",
    "\"\"\"\n",
    "\n",
    "# Display results\n",
    "print(\"Q1: Lasso Regression Overview\")\n",
    "print(lasso_regression_summary)\n",
    "\n",
    "print(\"\\nQ2: Advantage of Lasso Regression in Feature Selection\")\n",
    "print(feature_selection_advantage)\n",
    "\n",
    "print(\"\\nQ3: Interpreting Coefficients of a Lasso Regression Model\")\n",
    "print(coefficient_interpretation)\n",
    "\n",
    "print(\"\\nQ4: Tuning Parameters in Lasso Regression\")\n",
    "print(tuning_parameters)\n",
    "\n",
    "print(\"\\nQ5: Lasso Regression for Non-Linear Problems\")\n",
    "print(non_linear_regression_summary)\n",
    "\n",
    "print(\"\\nQ6: Difference Between Ridge and Lasso Regression\")\n",
    "print(ridge_lasso_comparison)\n",
    "\n",
    "print(\"\\nQ7: Handling Multicollinearity with Lasso Regression\")\n",
    "print(multicollinearity_summary)\n",
    "\n",
    "print(\"\\nQ8: Choosing the Optimal Regularization Parameter (lambda)\")\n",
    "print(optimal_lambda_selection)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
