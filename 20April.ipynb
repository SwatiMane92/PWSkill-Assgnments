{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ba8ae-ca2a-4265-93dd-7ba481cb3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the KNN algorithm?\n",
    "\n",
    "# The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric, and lazy learning algorithm \n",
    "# used for classification and regression tasks. It works by finding the 'K' nearest data points \n",
    "# (neighbors) to a query point and predicting the label (classification) or value (regression) based \n",
    "# on the majority vote (for classification) or the average of the neighbors (for regression).\n",
    "\n",
    "# Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "# The value of K is chosen based on experimentation, often using cross-validation to evaluate \n",
    "# different K values. A small K value can be sensitive to noise, while a large K value may smooth out \n",
    "# the decision boundary too much. Common values are K=3, 5, or 7.\n",
    "\n",
    "# Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "# KNN Classifier: Used for classification tasks. It predicts the class label based on the majority vote \n",
    "# of the nearest neighbors.\n",
    "# KNN Regressor: Used for regression tasks. It predicts a continuous value based on the average of the \n",
    "# nearest neighbors.\n",
    "\n",
    "# Q4. How do you measure the performance of KNN?\n",
    "\n",
    "# Performance of KNN can be measured using various metrics:\n",
    "# - For classification: Accuracy, Precision, Recall, F1 Score, and Confusion Matrix.\n",
    "# - For regression: Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared.\n",
    "\n",
    "# Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "# The curse of dimensionality refers to the phenomenon where the distance between points in a high-dimensional \n",
    "# space becomes less meaningful. As the number of dimensions increases, the volume of the space increases \n",
    "# exponentially, making it difficult to identify nearest neighbors effectively. This leads to decreased \n",
    "# performance of the KNN algorithm.\n",
    "\n",
    "# Q6. How do you handle missing values in KNN?\n",
    "\n",
    "# Missing values can be handled by:\n",
    "# - Imputing the missing values using the mean, median, or mode of the available data.\n",
    "# - Using KNN itself to impute missing values by finding the K-nearest neighbors for the data point \n",
    "# with missing values and imputing the missing data based on the neighbors' values.\n",
    "\n",
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor.\n",
    "\n",
    "# KNN Classifier:\n",
    "# - Better suited for categorical outcomes.\n",
    "# - Works well when the decision boundary is simple.\n",
    "# - Sensitive to the choice of K and distance metric.\n",
    "\n",
    "# KNN Regressor:\n",
    "# - Better suited for continuous outcomes.\n",
    "# - Sensitive to outliers, as it averages the neighbors' values.\n",
    "# - The performance depends on the distribution of the data and the value of K.\n",
    "\n",
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks?\n",
    "\n",
    "# Strengths:\n",
    "# - Simple and easy to understand.\n",
    "# - Non-parametric, making no assumptions about the data distribution.\n",
    "# - Effective with well-separated data.\n",
    "\n",
    "# Weaknesses:\n",
    "# - Computationally expensive, especially with large datasets.\n",
    "# - Sensitive to irrelevant features and the curse of dimensionality.\n",
    "# - Performance degrades with noisy or imbalanced data.\n",
    "\n",
    "# These weaknesses can be addressed by:\n",
    "# - Feature scaling (normalization or standardization).\n",
    "# - Dimensionality reduction techniques (e.g., PCA).\n",
    "# - Using efficient data structures like KD-trees for faster nearest neighbor search.\n",
    "\n",
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "# Euclidean Distance:\n",
    "# - Measures the straight-line (L2 norm) distance between two points in space.\n",
    "# - Suitable for continuous and well-separated data.\n",
    "\n",
    "# Manhattan Distance:\n",
    "# - Measures the distance between two points along the axes (L1 norm).\n",
    "# - Suitable for grid-like paths and high-dimensional data.\n",
    "\n",
    "# Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "# Feature scaling is crucial in KNN because the algorithm relies on distance measurements to find nearest \n",
    "# neighbors. If features are on different scales (e.g., one feature ranges from 0 to 1, while another ranges \n",
    "# from 0 to 1000), the distance calculations will be dominated by the feature with the larger scale. \n",
    "# Scaling ensures that all features contribute equally to the distance measurement.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
