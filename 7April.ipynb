{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b799fa7-ddc9-4985-b86d-d3761e4c986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1. Relationship between polynomial functions and kernel functions:\n",
      "Polynomial functions can be used as kernel functions in machine learning algorithms, specifically in SVM.\n",
      "The polynomial kernel function transforms the input space into a higher-dimensional space, allowing the SVM to find a hyperplane that can separate the data more effectively.\n",
      "For a polynomial kernel, the function is usually of the form: (gamma * <x, x'> + coef0) ^ degree\n",
      "\n",
      "Q2. Implementing an SVM with a polynomial kernel:\n",
      "Accuracy of polynomial kernel SVM: 1.00\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "\n",
      "Q3. Effect of epsilon on the number of support vectors in SVR:\n",
      "Increasing the value of epsilon in SVR makes the model less sensitive to small changes in the training data.\n",
      "This can result in fewer support vectors, as the model becomes less likely to fit the noise in the training data.\n",
      "\n",
      "Q4. Effect of kernel function, C, epsilon, and gamma in SVR:\n",
      "1. Kernel Function: Determines the type of transformation applied to the data. Common kernels include linear, polynomial, and RBF.\n",
      "   - Polynomial: Useful for capturing non-linear relationships.\n",
      "   - RBF: Effective for capturing complex patterns in high-dimensional space.\n",
      "2. C Parameter: Controls the trade-off between achieving a low error on the training data and minimizing the model complexity.\n",
      "   - A higher C tries to fit the training data better but may lead to overfitting.\n",
      "   - A lower C results in a simpler model and may lead to underfitting.\n",
      "3. Epsilon Parameter: Defines a margin of tolerance where no penalty is given for errors.\n",
      "   - Larger epsilon values allow for more tolerance and fewer support vectors.\n",
      "   - Smaller epsilon values make the model more sensitive to errors.\n",
      "4. Gamma Parameter: Defines how far the influence of a single training example reaches.\n",
      "   - High gamma values mean a small radius of influence, leading to more complex models.\n",
      "   - Low gamma values mean a larger radius of influence, leading to smoother models.\n",
      "\n",
      "Accuracy of linear SVM: 0.98\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from joblib import dump\n",
    "\n",
    "# Q1. Relationship between polynomial functions and kernel functions\n",
    "print(\"Q1. Relationship between polynomial functions and kernel functions:\")\n",
    "print(\"Polynomial functions can be used as kernel functions in machine learning algorithms, specifically in SVM.\")\n",
    "print(\"The polynomial kernel function transforms the input space into a higher-dimensional space, allowing the SVM to find a hyperplane that can separate the data more effectively.\")\n",
    "print(\"For a polynomial kernel, the function is usually of the form: (gamma * <x, x'> + coef0) ^ degree\")\n",
    "\n",
    "# Q2. Implementing SVM with a polynomial kernel in Python using Scikit-learn\n",
    "print(\"\\nQ2. Implementing an SVM with a polynomial kernel:\")\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Use only two classes for simplicity\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the SVC classifier with a polynomial kernel\n",
    "poly_svc = SVC(kernel='poly', degree=3, C=1.0, gamma='scale')\n",
    "poly_svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred_poly = poly_svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
    "print(f\"Accuracy of polynomial kernel SVM: {accuracy_poly:.2f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_poly))\n",
    "\n",
    "# Q3. Effect of epsilon on the number of support vectors in SVR\n",
    "print(\"\\nQ3. Effect of epsilon on the number of support vectors in SVR:\")\n",
    "print(\"Increasing the value of epsilon in SVR makes the model less sensitive to small changes in the training data.\")\n",
    "print(\"This can result in fewer support vectors, as the model becomes less likely to fit the noise in the training data.\")\n",
    "\n",
    "# Q4. Choice of kernel function, C parameter, epsilon, and gamma in SVR\n",
    "print(\"\\nQ4. Effect of kernel function, C, epsilon, and gamma in SVR:\")\n",
    "print(\"1. Kernel Function: Determines the type of transformation applied to the data. Common kernels include linear, polynomial, and RBF.\")\n",
    "print(\"   - Polynomial: Useful for capturing non-linear relationships.\")\n",
    "print(\"   - RBF: Effective for capturing complex patterns in high-dimensional space.\")\n",
    "print(\"2. C Parameter: Controls the trade-off between achieving a low error on the training data and minimizing the model complexity.\")\n",
    "print(\"   - A higher C tries to fit the training data better but may lead to overfitting.\")\n",
    "print(\"   - A lower C results in a simpler model and may lead to underfitting.\")\n",
    "print(\"3. Epsilon Parameter: Defines a margin of tolerance where no penalty is given for errors.\")\n",
    "print(\"   - Larger epsilon values allow for more tolerance and fewer support vectors.\")\n",
    "print(\"   - Smaller epsilon values make the model more sensitive to errors.\")\n",
    "print(\"4. Gamma Parameter: Defines how far the influence of a single training example reaches.\")\n",
    "print(\"   - High gamma values mean a small radius of influence, leading to more complex models.\")\n",
    "print(\"   - Low gamma values mean a larger radius of influence, leading to smoother models.\")\n",
    "\n",
    "# Q5. Assignment Implementation\n",
    "# Load a dataset\n",
    "# For this example, using the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the SVC classifier\n",
    "svc = SVC(kernel='linear')  # Start with a linear kernel\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy of linear SVM: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf'],\n",
    "    'degree': [2, 3, 4],  # Only used if kernel='poly'\n",
    "    'gamma': ['scale', 'auto']  # Only used if kernel='rbf'\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and performance\n",
    "print(\"\\nBest parameters found by GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "dump(best_svc, 'best_svc_model.joblib')\n",
    "print(\"\\nTrained classifier saved to 'best_svc_model.joblib'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc046e96-6202-4608-9139-9b6a164ff04d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
