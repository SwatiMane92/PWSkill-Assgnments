{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68df336-78f7-416c-aaeb-79cf9d4a1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the Wine Quality dataset\n",
    "wine_data = pd.read_csv('winequality-red.csv')  # Replace with the path to your dataset\n",
    "\n",
    "# Q1: Key features of the Wine Quality dataset\n",
    "key_features = wine_data.columns\n",
    "feature_importance = \"\"\"\n",
    "The key features of the wine quality dataset include:\n",
    "1. Fixed Acidity: Affects the taste and overall quality. High acidity can make the wine taste sour.\n",
    "2. Volatile Acidity: High levels can indicate spoilage. It impacts the wine's flavor and quality.\n",
    "3. Citric Acid: Adds freshness to the wine and helps balance the acidity.\n",
    "4. Residual Sugar: Affects sweetness. Higher levels can lead to a sweeter taste.\n",
    "5. Chlorides: Contributes to saltiness. High levels can impact the flavor negatively.\n",
    "6. Free Sulfur Dioxide: Acts as a preservative. Helps prevent oxidation and spoilage.\n",
    "7. Total Sulfur Dioxide: Total amount of sulfur compounds. High levels may affect taste and quality.\n",
    "8. Density: Indicates the amount of dissolved substances. Higher density can affect the mouthfeel.\n",
    "9. pH: Measures acidity. The balance of pH impacts the wine's taste and preservation.\n",
    "10. Sulphates: Contributes to the wine's stability and taste.\n",
    "11. Alcohol: Influences the body and taste. Higher alcohol content can enhance flavors and balance acidity.\n",
    "12. Quality: The target variable representing the quality rating of the wine.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Q1: Key Features of the Wine Quality Dataset\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Q2: Handling missing data and imputation techniques\n",
    "# Check for missing values\n",
    "missing_data = wine_data.isnull().sum()\n",
    "print(\"Missing Data:\")\n",
    "print(missing_data)\n",
    "\n",
    "# Impute missing data\n",
    "imputer = SimpleImputer(strategy='mean')  # Using mean imputation as an example\n",
    "wine_data_imputed = pd.DataFrame(imputer.fit_transform(wine_data), columns=wine_data.columns)\n",
    "\n",
    "imputation_summary = \"\"\"\n",
    "Different imputation techniques:\n",
    "1. Mean Imputation: Simple and works well if data is missing at random but can introduce bias.\n",
    "2. Median Imputation: Useful if data is skewed. Less sensitive to outliers.\n",
    "3. Mode Imputation: Suitable for categorical data. Replaces missing values with the most frequent value.\n",
    "4. K-Nearest Neighbors Imputation: Uses similar data points to impute missing values but can be computationally expensive.\n",
    "5. Predictive Imputation: Uses a model to predict missing values. More complex but can be more accurate.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nQ2: Handling Missing Data and Imputation Techniques\")\n",
    "print(imputation_summary)\n",
    "\n",
    "# Q3: Factors affecting students' performance\n",
    "factors = \"\"\"\n",
    "Key factors affecting students' performance include:\n",
    "1. Study Hours: More study time generally improves performance.\n",
    "2. Attendance: Regular attendance is often associated with better performance.\n",
    "3. Family Background: Parental support and education level can impact academic achievement.\n",
    "4. Health: Good health can lead to better focus and performance.\n",
    "5. Extracurricular Activities: Participation can either positively or negatively impact academic performance.\n",
    "6. Stress Levels: High stress can negatively affect performance.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nQ3: Factors Affecting Students' Performance\")\n",
    "print(factors)\n",
    "\n",
    "# Q4: Feature engineering for student performance dataset\n",
    "# Example: Transforming study hours into categorical bins\n",
    "student_data = pd.read_csv('student_performance.csv')  # Replace with the path to your dataset\n",
    "student_data['Study Hours'] = pd.cut(student_data['Study Hours'], bins=[0, 2, 4, 6, 8, 10], labels=['0-2', '2-4', '4-6', '6-8', '8-10'])\n",
    "\n",
    "print(\"\\nQ4: Feature Engineering for Student Performance Dataset\")\n",
    "print(student_data.head())\n",
    "\n",
    "# Q5: Exploratory Data Analysis (EDA) for the Wine Quality dataset\n",
    "# Distribution of each feature\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(wine_data.columns):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    sns.histplot(wine_data[col], kde=True)\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check normality using Shapiro-Wilk test\n",
    "normality_tests = {col: shapiro(wine_data[col].dropna())[1] for col in wine_data.columns}\n",
    "non_normal_features = [col for col, p in normality_tests.items() if p < 0.05]\n",
    "\n",
    "normality_summary = \"\"\"\n",
    "Features exhibiting non-normality:\n",
    "{}\n",
    "Possible transformations to improve normality:\n",
    "1. Log Transformation: Useful for right-skewed distributions.\n",
    "2. Square Root Transformation: Helps reduce right skewness.\n",
    "3. Box-Cox Transformation: Suitable for various types of non-normality.\n",
    "4. Yeo-Johnson Transformation: Handles both positive and negative skewness.\n",
    "\"\"\".format(non_normal_features)\n",
    "\n",
    "print(\"\\nQ5: Exploratory Data Analysis (EDA) and Normality Testing\")\n",
    "print(normality_summary)\n",
    "\n",
    "# Q6: Apply PCA for dimensionality reduction\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(wine_data_imputed.drop('quality', axis=1))  # Scale features\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Number of principal components to retain\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "num_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "pca_summary = \"\"\"\n",
    "PCA Results:\n",
    "- Minimum number of principal components needed to explain 95% of the variance: {}\n",
    "\"\"\".format(num_components)\n",
    "\n",
    "print(\"\\nQ6: PCA for Dimensionality Reduction\")\n",
    "print(pca_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
